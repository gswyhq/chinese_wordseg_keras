{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import sys\n",
    "from os import path\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from pickle import dump,load\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# SogouC.reduced.zip; https://pan.baidu.com/s/1pLrORJt\n",
    "rootdir = './data/SogouC.reduced/Reduced'\n",
    "dirs = os.listdir(rootdir)\n",
    "dirs = [path.join(rootdir,f) for f in dirs if f.startswith('C')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(x):\n",
    "    with open(x) as f:\n",
    "        res = [t for t in f.readlines()]\n",
    "        return ''.join(res)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def to_df(dirs):\n",
    "    text_t = {}\n",
    "    for i, d in enumerate(dirs):\n",
    "        files = os.listdir(d)\n",
    "        files = [path.join(d, x) for x in files if x.endswith('txt') and not x.startswith('.')]\n",
    "        text_t[i] = [load_txt(f) for f in files]\n",
    "\n",
    "    # to dataframe\n",
    "\n",
    "    flen = [len(t) for t in text_t.values()]\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    labels = np.repeat(list(text_t.keys()),flen)\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    # flatter nested list\n",
    "\n",
    "    merged = list(itertools.chain.from_iterable(text_t.values()))\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'label': labels, 'txt': merged})\n",
    "    df.head()\n",
    "    df['seg_word'] = df.txt.map(cutchar)\n",
    "\n",
    "    return df\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# cut character\n",
    "def cutchar(x):\n",
    "    words = list(x)\n",
    "    return ' '.join(words)\n",
    "\n",
    "df = to_df(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>word</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6393</th>\n",
       "      <td>4268470</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>886090</td>\n",
       "      <td>，</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>698338</td>\n",
       "      <td>的</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>411205</td>\n",
       "      <td>。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>224226</td>\n",
       "      <td>一</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         freq word  idx\n",
       "6393  4268470    \u0000    0\n",
       "372    886090    ，    1\n",
       "4133   698338    的    2\n",
       "3586   411205    。    3\n",
       "2855   224226    一    4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 探索 转成nltk需要格式,建立list\n",
    "txt = df['seg_word'].values\n",
    "txtnltk = []\n",
    "for sent in txt:\n",
    "    temp = [w for w in sent.split()]\n",
    "    txtnltk.extend(temp)\n",
    "\n",
    "# nltk\n",
    "corpus = nltk.Text(txtnltk)\n",
    "\n",
    "# 词频\n",
    "fdist = FreqDist(corpus)\n",
    "# w = fdist.keys()\n",
    "# v = fdist.values()\n",
    "w, v = [], []\n",
    "for k1, v1 in fdist.items():\n",
    "    w.append(k1)\n",
    "    v.append(v1)\n",
    "freqdf = pd.DataFrame({'word':w,'freq':v})\n",
    "freqdf.sort_values('freq',ascending =False, inplace=True)\n",
    "freqdf['idx'] = np.arange(len(v))\n",
    "freqdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存word2idx到文件\n",
      "保存idx2word到文件\n",
      "0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:37: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word2idx = {c:i for c, i in zip(freqdf.word, freqdf.idx)}\n",
    "idx2word = dict((i, c) for c, i in zip(freqdf.word, freqdf.idx))\n",
    "\n",
    "print('保存word2idx到文件')\n",
    "dump(word2idx, open('word2idx.pickle', 'wb'))\n",
    "print('保存idx2word到文件')\n",
    "dump(idx2word, open('idx2word.pickle', 'wb'))\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# 保持字符串，为生成词向量准备\n",
    "all_news_wv = []\n",
    "for news in txt:\n",
    "    all_news_wv.append([x for x in news.split() ])\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "# word2vec\n",
    "def trainW2V(corpus, epochs=50, num_features = 100,\n",
    "             min_word_count = 1, num_workers = 4,\n",
    "             context = 10, sample = 1e-5):\n",
    "    global w2v\n",
    "    w2v = word2vec.Word2Vec(workers = num_workers,\n",
    "                          sample = sample,\n",
    "                          size = num_features,\n",
    "                          min_count=min_word_count,\n",
    "                          window = context)\n",
    "    np.random.shuffle(corpus)\n",
    "    w2v.build_vocab(corpus)\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch, end=' ')\n",
    "        np.random.shuffle(corpus)\n",
    "        w2v.train(corpus, epochs=w2v.iter,total_examples=w2v.corpus_count)\n",
    "        w2v.alpha *= 0.9\n",
    "        w2v.min_alpha = w2v.alpha\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# word2vec\n",
    "trainW2V(all_news_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存字向量到文件\n"
     ]
    }
   ],
   "source": [
    "# 保存词向量lookup矩阵，按idx位置存放。目的是保存词频，也可以直接使用w2v.index2word\n",
    "init_weight_wv = []\n",
    "for i in range(freqdf.shape[0]):\n",
    "    init_weight_wv.append(w2v[idx2word[i]])\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "print('保存字向量到文件')\n",
    "dump(init_weight_wv, open('init_weight.pickle', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6892, 6892, 6892, 19, 11, 52, 222],\n",
       " [6892, 6892, 19, 11, 52, 222, 81],\n",
       " [6892, 19, 11, 52, 222, 81, 31],\n",
       " [19, 11, 52, 222, 81, 31, 276],\n",
       " [11, 52, 222, 81, 31, 276, 5],\n",
       " [52, 222, 81, 31, 276, 5, 4],\n",
       " [222, 81, 31, 276, 5, 4, 84],\n",
       " [81, 31, 276, 5, 4, 84, 189],\n",
       " [31, 276, 5, 4, 84, 189, 220],\n",
       " [276, 5, 4, 84, 189, 220, 411],\n",
       " [5, 4, 84, 189, 220, 411, 1],\n",
       " [4, 84, 189, 220, 411, 1, 78],\n",
       " [84, 189, 220, 411, 1, 78, 703],\n",
       " [189, 220, 411, 1, 78, 703, 106],\n",
       " [220, 411, 1, 78, 703, 106, 664],\n",
       " [411, 1, 78, 703, 106, 664, 2],\n",
       " [1, 78, 703, 106, 664, 2, 137],\n",
       " [78, 703, 106, 664, 2, 137, 430],\n",
       " [703, 106, 664, 2, 137, 430, 224],\n",
       " [106, 664, 2, 137, 430, 224, 5],\n",
       " [664, 2, 137, 430, 224, 5, 10],\n",
       " [2, 137, 430, 224, 5, 10, 57],\n",
       " [137, 430, 224, 5, 10, 57, 62],\n",
       " [430, 224, 5, 10, 57, 62, 87],\n",
       " [224, 5, 10, 57, 62, 87, 2],\n",
       " [5, 10, 57, 62, 87, 2, 189],\n",
       " [10, 57, 62, 87, 2, 189, 220],\n",
       " [57, 62, 87, 2, 189, 220, 411],\n",
       " [62, 87, 2, 189, 220, 411, 1],\n",
       " [87, 2, 189, 220, 411, 1, 319],\n",
       " [2, 189, 220, 411, 1, 319, 445],\n",
       " [189, 220, 411, 1, 319, 445, 100],\n",
       " [220, 411, 1, 319, 445, 100, 5],\n",
       " [411, 1, 319, 445, 100, 5, 194],\n",
       " [1, 319, 445, 100, 5, 194, 860],\n",
       " [319, 445, 100, 5, 194, 860, 108],\n",
       " [445, 100, 5, 194, 860, 108, 100],\n",
       " [100, 5, 194, 860, 108, 100, 2],\n",
       " [5, 194, 860, 108, 100, 2, 1134],\n",
       " [194, 860, 108, 100, 2, 1134, 32],\n",
       " [860, 108, 100, 2, 1134, 32, 2],\n",
       " [108, 100, 2, 1134, 32, 2, 22],\n",
       " [100, 2, 1134, 32, 2, 22, 35],\n",
       " [2, 1134, 32, 2, 22, 35, 1038],\n",
       " [1134, 32, 2, 22, 35, 1038, 3],\n",
       " [32, 2, 22, 35, 1038, 3, 6892],\n",
       " [2, 22, 35, 1038, 3, 6892, 6892],\n",
       " [22, 35, 1038, 3, 6892, 6892, 6892]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 读取数据，将格式进行转换为带四种标签 S B M E\n",
    "input_file = './data/icwb2-data/training/msr_training.utf8'\n",
    "output_file = './data/icwb2-data/training/msr_training.tagging.utf8'\n",
    "\n",
    "\n",
    "# 用于字符标记的4个标签：B（开始），E（结束），M（中），S（单）\n",
    "\n",
    "def character_tagging(input_file, output_file):\n",
    "    input_data = codecs.open(input_file, 'r', 'utf-8')\n",
    "    output_data = codecs.open(output_file, 'w', 'utf-8')\n",
    "    for line in input_data.readlines():\n",
    "        word_list = line.strip().split()\n",
    "        for word in word_list:\n",
    "            if len(word) == 1:\n",
    "                output_data.write(word + \"/S \")\n",
    "            else:\n",
    "                output_data.write(word[0] + \"/B \")\n",
    "                for w in word[1:len(word)-1]:\n",
    "                    output_data.write(w + \"/M \")\n",
    "                output_data.write(word[len(word)-1] + \"/E \")\n",
    "        output_data.write(\"\\n\")\n",
    "    input_data.close()\n",
    "    output_data.close()\n",
    "\n",
    "character_tagging(input_file, output_file)\n",
    "\n",
    "\n",
    "\n",
    "# 定义'U'为未登陆新字, 空格为两头padding用途，并增加两个相应的向量表示\n",
    "char_num = len(init_weight_wv)\n",
    "idx2word[char_num] = u'U'\n",
    "word2idx[u'U'] = char_num\n",
    "idx2word[char_num+1] = u' '\n",
    "word2idx[u' '] = char_num+1\n",
    "\n",
    "init_weight_wv.append(np.random.randn(100,))\n",
    "init_weight_wv.append(np.zeros(100,))\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "# 分离word 和 label\n",
    "with open(output_file) as f:\n",
    "    lines = f.readlines()\n",
    "    train_line = [[w[0] for w in line.split()] for line in lines]\n",
    "    train_label = [w[2] for line in lines for w in line.split()]\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# 文档转数字list\n",
    "def sent2num(sentence, word2idx = word2idx, context = 7):\n",
    "    predict_word_num = []\n",
    "    for w in sentence:\n",
    "        # 文本中的字如果在词典中则转为数字，如果不在则设置为'U\n",
    "        if w in word2idx:\n",
    "            predict_word_num.append(word2idx[w])\n",
    "        else:\n",
    "            predict_word_num.append(word2idx[u'U'])\n",
    "    # 首尾padding\n",
    "    num = len(predict_word_num)\n",
    "    pad = int((context-1)*0.5)\n",
    "    for i in range(pad):\n",
    "        predict_word_num.insert(0,word2idx[u' '] )\n",
    "        predict_word_num.append(word2idx[u' '] )\n",
    "    train_x = []\n",
    "    for i in range(num):\n",
    "        train_x.append(predict_word_num[i:i+context])\n",
    "    return train_x\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "# 输入字符list，输出数字list\n",
    "sent2num(train_line[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4050469\n",
      "4050469\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将所有训练文本转成数字list\n",
    "train_word_num = []\n",
    "for line in train_line:\n",
    "    train_word_num.extend(sent2num(line))\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "print(len(train_word_num))\n",
    "print(len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pickle import dump,load\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Reshape, Flatten ,Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import Callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 2, 'B': 0, 'E': 1, 'S': 3}\n",
      "{0: 'B', 1: 'E', 2: 'M', 3: 'S'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3645422 train sequences\n",
      "405047 test sequences\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_classes = len(np.unique(train_label))\n",
    "\n",
    "# 建立两个字典\n",
    "label_dict = dict(zip(np.unique(train_label), range(4)))\n",
    "num_dict = {n:l  for l,n  in label_dict.items()}\n",
    "print(label_dict)\n",
    "print(num_dict)\n",
    "# 将目标变量转为数字\n",
    "train_label = [label_dict[y] for y in train_label]\n",
    "\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "\n",
    "# 切分数据集\n",
    "train_X, test_X, train_y, test_y = train_test_split(train_word_num, train_label , train_size=0.9, random_state=1)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "\n",
    "Y_train = np_utils.to_categorical(train_y, nb_classes)\n",
    "Y_test = np_utils.to_categorical(test_y, nb_classes)\n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "\n",
    "print(len(train_X), 'train sequences')\n",
    "print(len(test_X), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7, 100)            689300    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               70100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 759,804\n",
      "Trainable params: 759,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 初始字向量格式准备\n",
    "init_weight = [np.array(init_weight_wv)]\n",
    "\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "maxfeatures = init_weight[0].shape[0] # 词典大小\n",
    "\n",
    "\n",
    "# In[87]:\n",
    "\n",
    "\n",
    "# 一个普通的单隐层神经网络，输入层700，隐藏层100，输出层4\n",
    "# 迭代时同时更新神经网络权重，以及词向量\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# 词向量初始化，输入维度：词典大小，输出维度：词向量100\n",
    "model.add(Embedding(maxfeatures, 100,weights=init_weight,input_length=7)) # 使用初使词向量可以增加准确率\n",
    "# Embedding: 将正整数（索引值）转换为固定尺寸的稠密向量。 例如： [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]; 该层只能用作模型中的第一层。\n",
    "\n",
    "model.add(Flatten())\n",
    "# Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。\n",
    "# eg: (None, 64, 32, 32) --> (None, 65536)\n",
    "\n",
    "model.add(Dense(100, input_dim=700, activation='relu'))\n",
    "model.add(Dropout(0.5))  # 为输入数据施加Dropout。Dropout将在训练过程中每次更新参数时按一定概率（rate）随机断开输入神经元，Dropout层用于防止过拟合。\n",
    "\n",
    "# model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes, input_dim=100, activation='softmax'))\n",
    "# model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:34: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3280879 samples, validate on 364543 samples\n",
      "Epoch 1/1\n",
      "3280879/3280879 [==============================] - 106s 32us/step - loss: 0.5009 - val_loss: 0.3385\n",
      "405047/405047 [==============================] - 2s 4us/step\n",
      "Test score: 0.3393762092336496\n",
      "Test accuracy: 0.8757773789214585\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EarlyStopping(Callback):\n",
    "    def __init__(self, patience=0, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = np.Inf\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # if not self.params['do_validation']:\n",
    "        #     warnings.warn(\"Early stopping requires validation data!\", RuntimeWarning)\n",
    "\n",
    "        cur_val_loss = logs.get('val_loss')\n",
    "        if cur_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = cur_val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Epoch %05d: early stopping\" % (epoch))\n",
    "                self.model.stop_training = True\n",
    "            self.wait += 1\n",
    "\n",
    "\n",
    "# In[144]:\n",
    "\n",
    "\n",
    "# train_X, test_X, Y_train, Y_test\n",
    "print(\"Train...\")\n",
    "earlystop = EarlyStopping(patience=0, verbose=1)\n",
    "result = model.fit(np.array(train_X), Y_train, batch_size=batch_size, nb_epoch=1,\n",
    "          validation_split=0.1,callbacks=[earlystop])\n",
    "\n",
    "\n",
    "# In[145]:\n",
    "\n",
    "\n",
    "score = earlystop.model.evaluate(np.array(test_X), Y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "\n",
    "\n",
    "# In[146]:\n",
    "\n",
    "\n",
    "# test数据集，准确率0.94\n",
    "classes = earlystop.model.predict_classes(np.array(test_X), batch_size=batch_size)\n",
    "# acc = np_utils.accuracy(classes, test_y) # 要用没有转换前的y\n",
    "acc = sum([1 if k==v else 0 for k, v in zip(classes, test_y)])/len(test_y)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6892, 6892, 6892, 16, 63, 611, 369], [6892, 6892, 16, 63, 611, 369, 729], [6892, 16, 63, 611, 369, 729, 253], [16, 63, 611, 369, 729, 253, 449], [63, 611, 369, 729, 253, 449, 50]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from pickle import dump,load\n",
    "\n",
    "\n",
    "# - 步骤4：用test文本进行预测，评估效果\n",
    "\n",
    "temp_txt = u'国家食药监总局发布通知称，酮康唑口服制剂因存在严重肝毒性不良反应，即日起停止生产销售使用。'\n",
    "temp_txt = list(temp_txt)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "temp_num = sent2num(temp_txt)\n",
    "print(temp_num[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国/B 家/M 食/M 药/M 监/M 总/M 局/E 发/B 布/E 通/B 知/E 称/S ，/S 酮/B 康/E 唑/B 口/E 服/B 制/E 剂/B 因/E 存/B 在/E 严/B 重/E 肝/B 毒/E 性/S 不/B 良/E 反/B 应/E ，/S 即/B 日/E 起/S 停/B 止/E 生/B 产/E 销/B 售/E 使/B 用/E 。/S\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 根据输入得到标注推断\n",
    "def predict_num(input_num,input_txt,\n",
    "                model = model,\n",
    "                label_dict=label_dict,\n",
    "                num_dict=num_dict):\n",
    "    input_num = np.array(input_num)\n",
    "    predict_prob = model.predict_proba(input_num)\n",
    "    predict_lable = model.predict_classes(input_num)\n",
    "    for i , lable in enumerate(predict_lable[:-1]):\n",
    "        # 如果是首字 ，不可为E, M\n",
    "        if i==0:\n",
    "            predict_prob[i, label_dict[u'E']] = 0\n",
    "            predict_prob[i, label_dict[u'M']] = 0\n",
    "        # 前字为B，后字不可为B,S\n",
    "        if lable == label_dict[u'B']:\n",
    "            predict_prob[i+1,label_dict[u'B']] = 0\n",
    "            predict_prob[i+1,label_dict[u'S']] = 0\n",
    "        # 前字为E，后字不可为M,E\n",
    "        if lable == label_dict[u'E']:\n",
    "            predict_prob[i+1,label_dict[u'M']] = 0\n",
    "            predict_prob[i+1,label_dict[u'E']] = 0\n",
    "        # 前字为M，后字不可为B,S\n",
    "        if lable == label_dict[u'M']:\n",
    "            predict_prob[i+1,label_dict[u'B']] = 0\n",
    "            predict_prob[i+1,label_dict[u'S']] = 0\n",
    "        # 前字为S，后字不可为M,E\n",
    "        if lable == label_dict[u'S']:\n",
    "            predict_prob[i+1,label_dict[u'M']] = 0\n",
    "            predict_prob[i+1,label_dict[u'E']] = 0\n",
    "        predict_lable[i+1] = predict_prob[i+1].argmax()\n",
    "    predict_lable_new = [num_dict[x]  for x in predict_lable]\n",
    "    result =  [w+'/' +l  for w, l in zip(input_txt,predict_lable_new)]\n",
    "    return ' '.join(result) + '\\n'\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "temp = predict_num(temp_num,temp_txt)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import sys\n",
    "from os import path\n",
    "from pickle import dump,load\n",
    "\n",
    "\n",
    "test_file = './data/icwb2-data/testing/msr_test.utf8'\n",
    "with open(test_file,'r') as f:\n",
    "    lines = f.readlines()\n",
    "    test_texts = [list(line.strip()) for line in lines]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "test_output = []\n",
    "for line in test_texts:\n",
    "    test_num = sent2num(line)\n",
    "    output_line = predict_num(test_num,input_txt=line)\n",
    "    test_output.append(output_line)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "with open('./data/icwb2-data/testing/msr_test_output.utf8','w') as f:\n",
    "    f.writelines(test_output)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "input_file = './data/icwb2-data/testing/msr_test_output.utf8'\n",
    "output_file = './data/icwb2-data/testing/msr_test.split.tag2word.utf8'\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "\n",
    "def character_2_word(input_file, output_file):\n",
    "    input_data = codecs.open(input_file, 'r', 'utf-8')\n",
    "    output_data = codecs.open(output_file, 'w', 'utf-8')\n",
    "    # 4 tags for character tagging: B(Begin), E(End), M(Middle), S(Single)\n",
    "    for line in input_data.readlines():\n",
    "        char_tag_list = line.strip().split()\n",
    "        for char_tag in char_tag_list:\n",
    "            char_tag_pair = char_tag.split('/')\n",
    "            char = char_tag_pair[0]\n",
    "            tag = char_tag_pair[1]\n",
    "            if tag == 'B':\n",
    "                output_data.write(' ' + char)\n",
    "            elif tag == 'M':\n",
    "                output_data.write(char)\n",
    "            elif tag == 'E':\n",
    "                output_data.write(char + ' ')\n",
    "            else: # tag == 'S'\n",
    "                output_data.write(' ' + char + ' ')\n",
    "        output_data.write(\"\\n\")\n",
    "    input_data.close()\n",
    "    output_data.close()\n",
    "\n",
    "character_2_word(input_file, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./data/icwb2-data/scripts/score data/icwb2-data/gold/msr_training_words.utf8 data/icwb2-data/gold/msr_test_gold.utf8 data/icwb2-data/testing/msr_test.split.tag2word.utf8 > deep.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剧烈\t\t\t\t\t\t\t\t剧烈\r\n",
      "的\t\t\t\t\t\t\t\t的\r\n",
      "碰撞\t\t\t\t\t\t\t\t碰撞\r\n",
      "，\t\t\t\t\t\t\t\t，\r\n",
      "都\t\t\t\t\t\t\t\t都\r\n",
      "将\t\t\t\t\t\t\t\t将\r\n",
      "激起\t\t\t\t\t\t\t\t激起\r\n",
      "一\t\t\t\t\t\t\t      |\t一团耀\r\n",
      "团\t\t\t\t\t\t\t      |\t眼\r\n",
      "耀眼\t\t\t\t\t\t\t      <\r\n",
      "的\t\t\t\t\t\t\t\t的\r\n",
      "火花\t\t\t\t\t\t\t\t火花\r\n",
      "，\t\t\t\t\t\t\t\t，\r\n",
      "这\t\t\t\t\t\t\t\t这\r\n",
      "就\t\t\t\t\t\t\t\t就\r\n",
      "是\t\t\t\t\t\t\t\t是\r\n",
      "崭新\t\t\t\t\t\t\t\t崭新\r\n",
      "的\t\t\t\t\t\t\t\t的\r\n",
      "计算机技术\t\t\t\t\t\t      |\t计算机\r\n",
      "\t\t\t\t\t\t\t      >\t技术\r\n",
      "和\t\t\t\t\t\t\t\t和\r\n",
      "古老\t\t\t\t\t\t\t\t古老\r\n",
      "的\t\t\t\t\t\t\t\t的\r\n",
      "中华\t\t\t\t\t\t\t\t中华\r\n",
      "文化\t\t\t\t\t\t\t\t文化\r\n",
      "的\t\t\t\t\t\t\t\t的\r\n",
      "对话\t\t\t\t\t\t\t\t对话\r\n",
      "。\t\t\t\t\t\t\t\t。\r\n",
      "INSERTIONS:\t2\r\n",
      "DELETIONS:\t2\r\n",
      "SUBSTITUTIONS:\t7\r\n",
      "NCHANGE:\t11\r\n",
      "NTRUTH:\t45\r\n",
      "NTEST:\t45\r\n",
      "TRUE WORDS RECALL:\t0.800\r\n",
      "TEST WORDS PRECISION:\t0.800\r\n",
      "=== SUMMARY:\r\n",
      "=== TOTAL INSERTIONS:\t4303\r\n",
      "=== TOTAL DELETIONS:\t4382\r\n",
      "=== TOTAL SUBSTITUTIONS:\t11187\r\n",
      "=== TOTAL NCHANGE:\t19872\r\n",
      "=== TOTAL TRUE WORD COUNT:\t106873\r\n",
      "=== TOTAL TEST WORD COUNT:\t106794\r\n",
      "=== TOTAL TRUE WORDS RECALL:\t0.854\r\n",
      "=== TOTAL TEST WORDS PRECISION:\t0.855\r\n",
      "=== F MEASURE:\t0.855\r\n",
      "=== OOV Rate:\t0.026\r\n",
      "=== OOV Recall Rate:\t0.586\r\n",
      "=== IV Recall Rate:\t0.862\r\n",
      "###\tdata/icwb2-data/testing/msr_test.split.tag2word.utf8\t4303\t4382\t11187\t19872\t106873\t106794\t0.854\t0.855\t0.855\t0.026\t0.586\t0.862\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 50 deep.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
